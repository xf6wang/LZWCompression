# LZWCompression
A lossles data compression tool written in C++ that implements the Lempel–Ziv–Welch algorithm. 

Algorithm: The idea is to map character sequences to a number equivalent. Intially, the dictionary is comprised of 256 single, 8 bit characters; the ASCII values. The input stream is parsed until the largest character sequence that has not been mapped to any dictionary entry is found. Then, a new entry and mapping (code, value pair) is created and inserted into the dictionary. As the dictionary begins to increment in size, the compression becomes more powerful as inreasingly larger sequences are able to be compressed. The result sequence is parsed and decompressed using its own dictionary. An additional entry is inserted each time a result is decoded, namely the concationation of the decoded symbol and the first character of the subsquently decoded symbol. (There is a cScSc situation where the second symbol does not map to any dictionary entry whereby the concationation becomes the decoded symbol appended with its own first character) The decoded dictionary exactly mirrors the dictionary created in the encoding process, allowing for lossless compression and decompression.

Optimizations: All optimizations are done in the storage of data. The core compression algorithm is efficient. The most expensive operations are in the search and insertion of entries. In the non - optimized manner, the ADT is simply a linked list. Insertion is a push operation, since sorting the list provides no benefits, so it is O(1). However the lookup is much more costly. On average, each lookup is O(N) but for the LZW algorithm, it must be ensured at each dictionary value is unique. Thus, when searching for the longest sequence that is not in the dictionary, the list must be traversed a multiple times including one time where the entire list is traversed to verify its uniqueness. This is done quite frequently and as the dictionary becomes larger due to more entry insertions, the lookup time is extremely expensive. 

Optimization 1: Binary Tree for encoding 
Storing the dictionary entries as a Binary Tree rather than a linked list reduces the lookup time but increases the insertion time. However, given how costly the lookup is for a linked list, the increased insertion times are welcome. This binary tree is unique in that due to the way the initial dictionary was inserted, it begins effectively as a splay tree rather and a balanced BST. Since no balancing was done, we can think of this tree as a linked list of binary trees! Why? The initial splay tree is effectively a linked list. Now we take an entry(ie. "He") we traverse until we hit "H" at which point we go one further(strcmp("H", "He") will be  < 0) because a character is valued higher than a null character. We get to "I" where we find "He" is less than "I" and thus inserted.The next time something(ie."Hel") approaches, it will reach the "He" node and be appended. We can see that as the number of entries increases(given large text and image documents they will be fairly random) we effectively have a small BST for each ASCII character, a linked list of binary trees! Lookup is much faster because we only need to look at 1 / 256 of the dictionary entries in one go (actually less because each BST can be traversed quickly as well) Now, both lookup and insertion are Θ(log n) operations.This reduces the encoding time from hours to under 1 second for a 1 MB file. 

Optimization 2: Frequency analysis of doubly linked list for decoding 
This idea of frequency analysis was taken from Fengyuan Zhang, an Associate Professor at the University of Beijing. The concept is to move entries that are more commonly used to the front of the list. Thus, the lookup time should be decreased if there large number of queries. However, this is still not as efficient as using a Binary Tree because the entire dictionary must still be traversed to verify that an entry does not exist. Because of this, as the dictionary becomes larger, the speed suffers. So why do this rather than use another Binary Tree? Consider, we have an sequence of codes (the result of encoding) from this we must look in the dictionary to find the corresponding mapping. If the BST is created based on the value mappings we still have to traverse the entire tree to lookup the mapping for a code. If we sort based on the number, every additional entry is incrementally one larger. Thus, we would effectively create a linked list anyways. With this in mind we will create a linked list which cannot be avoided in the decoding process but attempt to optimize it with strategic placement of dictionary entries. Every time a dictionary entry is accessed, we increment its usage and assess whether or not it has passed a certain threshold. If so, we insert it to the front of the list in the appropriate location. We sort the top 10 % of the entries and insert new entries just behind the threshold.This optimization reduces the runtime for a 1 MB file from 8 min to approximately 1 min. Not as good as the binary tree but better than a regular linked list.Admittedly, the decoding time is not very desirable compared to the encoding time.

TODO: 
Image and Video compression with the help of Boost libraries
Implement BST in the decoding process by periodically balancing the splay tree
